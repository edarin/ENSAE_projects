{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.special import gammaln\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Words\n",
    "W = np.array([0, 1, 2, 3,4])\n",
    "\n",
    "# D := document words\n",
    "X = np.array([\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 2,1],\n",
    "    [4],\n",
    "    [2,3,2,3],\n",
    "  [2,3,2,3,2]\n",
    "])\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 3\n",
    "alpha = 0.1\n",
    "beta = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_docs = len(X)\n",
    "n_words = len(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic = np.zeros((n_docs, n_topics)) #nmz number of words per topic for each doc\n",
    "word_topic = np.zeros((n_topics, n_words)) #nzw count of each word for each topic\n",
    "doc = np.zeros(n_docs) #nm\n",
    "topic = np.zeros(n_topics) #nz\n",
    "topics_peridx = {} # topic assigned for each word for each document \n",
    "topics_perw = {} # topic assigned for each word for each document\n",
    "\n",
    "for d in range(n_docs):\n",
    "    idx =0\n",
    "    for w in X[d,]:\n",
    "        t=np.random.randint(n_topics)\n",
    "        doc_topic[d, t] +=1 #\n",
    "        doc[d] +=1\n",
    "        word_topic[t,w] +=1\n",
    "        topic[t] +=1\n",
    "        topics_peridx[(d,idx)] = t\n",
    "        topics_perw[(d,w)] = t\n",
    "        idx +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_distrib(d,w):\n",
    "    left = (word_topic[:,w] + beta) / (topic + beta*n_words)\n",
    "    right = (doc_topic[d,:] + alpha) / (doc[d] + alpha*n_topics)\n",
    "    p_t = left*right\n",
    "    p_t /= np.sum(p_t)\n",
    "    return(p_t)\n",
    "\n",
    "def log_multi_beta(alpha, K=None):\n",
    "    \"\"\"\n",
    "    Logarithm of the multinomial beta function.\n",
    "    \"\"\"\n",
    "    if K is None:\n",
    "        # alpha is assumed to be a vector\n",
    "        return np.sum(gammaln(alpha)) - gammaln(np.sum(alpha))\n",
    "    else:\n",
    "        # alpha is assumed to be a scalar\n",
    "        return K * gammaln(alpha) - gammaln(K*alpha)\n",
    "\n",
    "def loglikelihood():\n",
    "    \"\"\"\n",
    "    Compute the likelihood that the model generated the data.\n",
    "    \"\"\"\n",
    "\n",
    "    loglike = 0\n",
    "\n",
    "    for t in range(n_topics):\n",
    "        loglike += log_multi_beta(word_topic[t,:]+beta)\n",
    "        loglike -= log_multi_beta(beta, n_words)\n",
    "\n",
    "    for d in range(n_docs):\n",
    "        loglike += log_multi_beta(doc_topic[d,:]+alpha)\n",
    "        loglike -= log_multi_beta(alpha, n_topics)\n",
    "    \n",
    "    return loglike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-46.72724496251951\n",
      "-42.66358559382525\n",
      "-41.24732156301417\n",
      "-33.06367036013793\n",
      "-36.4649343594338\n",
      "-34.23925836087318\n",
      "-33.06367036013793\n",
      "-34.23925836087318\n",
      "-33.06367036013793\n",
      "-33.06367036013793\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(10):\n",
    "    for d in range(n_docs):\n",
    "        idx =0\n",
    "        for w in X[d,]:        \n",
    "            t = topics_peridx[(d,idx)]\n",
    "            doc_topic[d, t] -=1 \n",
    "            doc[d] -=1\n",
    "            word_topic[t,w] -=1\n",
    "            topic[t] -=1\n",
    "\n",
    "            p_t = conditional_distrib(d,w)\n",
    "            t = np.random.multinomial(1,p_t)\n",
    "            t= t.argmax()\n",
    "\n",
    "            doc_topic[d, t] +=1 \n",
    "            doc[d] +=1\n",
    "            word_topic[t,w] +=1\n",
    "            topic[t] +=1\n",
    "            topics_peridx[(d,idx)] = t\n",
    "            topics_perw[(d,w)] =t\n",
    "\n",
    "            idx +=1\n",
    "\n",
    "    print(loglikelihood())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 5., 4., 0.],\n",
       "       [5., 0., 0., 0., 0.],\n",
       "       [3., 1., 1., 0., 1.]])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([0, 0, 0, 0, 0]), list([0, 0, 0, 2, 1]), list([4]),\n",
       "       list([2, 3, 2, 3]), list([2, 3, 2, 3, 2])], dtype=object)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
